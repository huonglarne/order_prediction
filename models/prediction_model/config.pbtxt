name: "prediction_model"
platform: "onnxruntime_onnx"
max_batch_size: 256
input
 {
    name: "input"
    data_type: TYPE_FP16
    dims: [-1]
  }

output {
    name: "output"
    data_type: TYPE_FP16
    dims: [-1, 32]
  }
instance_group [
  {
    kind: KIND_CPU
    count: 2
  }
]
dynamic_batching {
  preferred_batch_size: [4, 8, 16, 32, 64]
  max_queue_delay_microseconds: 300
}
